{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 31 17:28:47 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.07              Driver Version: 546.12       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070        On  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   51C    P5              12W / 200W |   1094MiB / 12282MiB |      2%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import torch\n",
    "import hopsworks\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT4Tokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/text-embedding-ada-002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoding(dataset, embedding_object):\n",
    "    decodings = []\n",
    "    for data in dataset[\"embeddings\"]:\n",
    "        decoded_text = embedding_object.decode(data)\n",
    "        decodings.append(decoded_text)\n",
    "\n",
    "    dataset_decoded = dataset.copy()\n",
    "    dataset_decoded[\"text\"] = decodings\n",
    "    dataset_decoded = dataset_decoded.drop(columns=[\"embeddings\"])\n",
    "    return dataset_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Multiple projects found. \n",
      "\n",
      "\t (1) id2223labs\n",
      "\t (2) calinp\n",
      "\n",
      "Enter project to access: 1\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/197784\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "hopsworks_project = hopsworks.login() \n",
    "fs = hopsworks_project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from Hopsworks, using ArrowFlight.   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "W20231231 18:00:31.720172 19619 status.cc:137] DoAction result was not fully consumed: Cancelled: Flight cancelled call, with message: CANCELLED. Detail: Cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (2.64s) \n",
      "Finished: Reading data from Hopsworks, using ArrowFlight (1.29s) \n"
     ]
    }
   ],
   "source": [
    "training_fg = fs.get_or_create_feature_group(\"news_sentiment_traindata\", version=1)\n",
    "test_fg = fs.get_or_create_feature_group(\"news_sentiment_testdata\", version=1)\n",
    "# get all of the data from the feature group\n",
    "training_features = training_fg.read()\n",
    "testing_features = test_fg.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_decoding(training_features, tokenizer)\n",
    "testing_data = get_decoding(testing_features, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                               text\n",
      "0          1  This S&P 500 comeback stock could ride higher ...\n",
      "1          0  Study Finds Most Packaged Foods Contain Danger...\n",
      "2          2  \"1/3 of all women in the world have experience...\n",
      "3          2  Top Earnings Tu 11/26 Pre: $ADI $AMWD $ANF $BB...\n",
      "4          0  About half of U.S. small businesses haven't pa...\n",
      "...      ...                                                ...\n",
      "12474      2  ... - December 22nd, 2023 (Trade Strategy For ...\n",
      "12475      1  2 Red-Hot Artificial Intelligence (AI) Stocks ...\n",
      "12476      1  Google paying $700M to settle antitrust allega...\n",
      "12477      2  Google Allows More App Payment Options in Anti...\n",
      "12478      0  Bull of the Day: Amazon.com, Inc. (AMZN). Amaz...\n",
      "\n",
      "[12479 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# print the 50th row\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return bert_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframes to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(training_data)\n",
    "test_dataset = Dataset.from_pandas(testing_data)\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef114f2df2f5483aa8429cb6932dd782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12479 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e1a0c3dade42b88265f90914b2612b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Most Americans Think They Won't Need Social Security. They're Wrong.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']['text'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compute_metrics(metric):\n",
    "    def compute_metrics(eval_pred): \n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "    return compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
    "label2id = {val: key for key, val in id2label.items()}\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True, num_labels=3,\n",
    "                                                             id2label=id2label, label2id=label2id)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "compute_metrics = get_compute_metrics(metric)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_sentiment_trainer\", \n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate= 2.754984679344267e-05,\n",
    "    save_total_limit=3,\n",
    "    seed=42,\n",
    "    lr_scheduler_type='constant_with_warmup',\n",
    "    warmup_steps=50,\n",
    "    max_steps=3000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    fp16=False,\n",
    "    eval_steps=250,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "tokenized_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=55)\n",
    "tokenized_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=55)\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init,\n",
    "    tokenizer=bert_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 47:55, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.444748</td>\n",
       "      <td>0.833760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.383488</td>\n",
       "      <td>0.861307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.353260</td>\n",
       "      <td>0.869635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>0.442490</td>\n",
       "      <td>0.870596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>0.868354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>0.381386</td>\n",
       "      <td>0.881166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.558913</td>\n",
       "      <td>0.875080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.179100</td>\n",
       "      <td>0.586923</td>\n",
       "      <td>0.881807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.574099</td>\n",
       "      <td>0.874439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.632827</td>\n",
       "      <td>0.883408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.701510</td>\n",
       "      <td>0.874760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.704389</td>\n",
       "      <td>0.885650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.254610068321228, metrics={'train_runtime': 2876.0624, 'train_samples_per_second': 16.689, 'train_steps_per_second': 1.043, 'total_flos': 1.2628654710690816e+16, 'train_loss': 0.254610068321228, 'epoch': 3.85})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11defbf34f7848e8aea07c6bacbcaa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0057fdd28f194aefaa1c0d0ebe377e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1703758864.ArtanisPC.7175.14:   0%|          | 0.00/27.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb29cac9b924d89a63a9d04acf09823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a719faad829144d49ed9a77c229e5a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Artanis1551/bert_sentiment_trainer/tree/main/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old model metric = 0.8856502242152466\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model metric = 0.8856502242152466\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"Artanis1551/bert_sentiment_trainer\")\n",
    "results = pipe(list(testing_data[\"text\"]))\n",
    "\n",
    "json_dict = json.loads(str(results).replace('\\'', '\\\"'))\n",
    "predictions = pd.DataFrame.from_dict(json_dict)\n",
    "\n",
    "predicted_labels = [pipe.model.config.label2id[x] for x in predictions['label']] \n",
    "\n",
    "old_accuracy = metric.compute(predictions=predicted_labels, references=test_dataset['label'])['accuracy']\n",
    "print(\"old model metric = \" + str(old_accuracy))\n",
    "\n",
    "new_accuracy = trainer.predict(tokenized_test_dataset).metrics[\"test_accuracy\"]\n",
    "print(\"new model metric = \" + str(new_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
